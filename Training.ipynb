{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c77dda",
   "metadata": {},
   "source": [
    "### For this notebook, you should use the custom version of transformers (see `readme.md` for how to install the environment)\n",
    "\n",
    "This notebook contains a ready for use code to train CamemBERT-bio + LESA on your own medical dataset. \n",
    "It only requires your dataset to be a list of samples: `{'tokens': [`$w_1, w_2, \\cdots, w_n$`], 'classes': int list}`\n",
    "\n",
    "In order to properly break the text into words, use the tools in the utils directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c65f6bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'lesa_venv' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from transformers import CamembertForTokenClassification_label, CamembertForMaskedLM_label, AutoTokenizer, AutoConfig\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "from utils import stats, decode_properly, custom_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab7d7f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomizedDataset(Dataset):\n",
    "    def __init__(self, dataset_list):\n",
    "        self.files = dataset_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.files[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df9086a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model_name, optim, lr, weight_decay, T_0=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters('model_name', 'optim', 'lr', 'weight_decay', 'T_0') \n",
    "        self.optim = optim   \n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.T_0 = T_0\n",
    "        self.name = model_name\n",
    "        if self.name == 'LESA_BertForMaskedLM':\n",
    "            self.model = CamembertForMaskedLM_label.from_pretrained('almanach/camembert-bio-base')\n",
    "            self.num_labels = self.model.lm_head.decoder.out_features\n",
    "        elif self.name == 'LESA_BertForTokClassif':\n",
    "            self.model = CamembertForTokenClassification_label.from_pretrained('almanach/camembert-bio-base')\n",
    "            self.num_labels = self.model.num_labels\n",
    "        else:\n",
    "            raise ValueError('model name:' + model_name + 'is unknown')\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        return self.model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                label_inputs = batch[\"label_inputs\"],\n",
    "                attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        \n",
    "    def training_step(self, batch):\n",
    "        out = self.forward(batch)\n",
    "\n",
    "        logits = out.logits\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()    \n",
    "        loss = loss_fn(logits.view(-1, self.num_labels), batch[\"labels\"].view(-1))\n",
    "\n",
    "        self.log(\"train/loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        out = self.forward(batch)\n",
    "        preds = torch.max(out.logits, -1).indices\n",
    "        \n",
    "        if self.name == 'LESA_BertForTokClassif':\n",
    "            f1 = custom_f1_score(batch[\"labels\"].view(-1).cpu().numpy(), preds.view(-1).cpu().numpy(),\n",
    "                 self.num_labels, average = \"macro\")\n",
    "            self.log(\"valid/f1\", f1, prog_bar=True, on_step=False, on_epoch=True)\n",
    "            \n",
    "        else:\n",
    "            y_true = torch.where(batch[\"labels\"].view(-1)==tokenizer.pad_token_id, -1, batch[\"labels\"].view(-1))\n",
    "            acc = (y_true==preds.view(-1)).sum()/(y_true >= 0).sum()\n",
    "            self.log(\"valid/acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "            \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        out = self.forward(batch)\n",
    "\n",
    "        return torch.max(out.logits, -1).indices\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.optim == 'AdamW':\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                            self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay\n",
    "                        )\n",
    "        elif self.optim == 'SGD':\n",
    "            optimizer = torch.optim.SGD(\n",
    "                            self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay\n",
    "                        )\n",
    "        else:\n",
    "            raise ValueError('Optim name: ' + self.optim + ' is unknown')\n",
    "        \n",
    "        if self.T_0 is None:\n",
    "            return optimizer\n",
    "        else:\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, self.T_0)\n",
    "            return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fe72bf",
   "metadata": {},
   "source": [
    "## Setting up the label related keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54b7db95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#key words for the sequence classification task\n",
    "key_words = np.array([['mot', 'patient', 'général'],\n",
    "                      ['FE', 'FR', 'systolique'],\n",
    "                      ['cardiaque', 'FC', 'fréquence'],\n",
    "                      ['diamètre', 'pulmonaire', 'APP'], \n",
    "                      ['pulsée', 'O2', 'sat'],\n",
    "                      ['APGAR', 'Naissance', 'terme'],\n",
    "                      ['gradient', 'pulmonaire', 'ventricule'],\n",
    "                      ['CIA', 'CIV', 'inlet']\n",
    "                     ])\n",
    "num_labels = 8 #['O', 'Cp', 'FC', 'D', 'SO2', 'AGPR', 'G', 'CI']\n",
    "num_words_per_label = 3\n",
    "label_inputs = torch.zeros((num_labels, num_words_per_label), dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "690c004b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SSLError",
     "evalue": "HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /almanach/camembert-bio-base/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    468\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/urllib3/connectionpool.py:1092\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m conn\u001b[39m.\u001b[39mis_closed:\n\u001b[0;32m-> 1092\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1094\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/urllib3/connection.py:635\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    628\u001b[0m         (\n\u001b[1;32m    629\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSystem time is way off (before \u001b[39m\u001b[39m{\u001b[39;00mRECENT_DATE\u001b[39m}\u001b[39;00m\u001b[39m). This will probably \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m         SystemTimeWarning,\n\u001b[1;32m    633\u001b[0m     )\n\u001b[0;32m--> 635\u001b[0m sock_and_verified \u001b[39m=\u001b[39m _ssl_wrap_socket_and_match_hostname(\n\u001b[1;32m    636\u001b[0m     sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    637\u001b[0m     cert_reqs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_reqs,\n\u001b[1;32m    638\u001b[0m     ssl_version\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mssl_version,\n\u001b[1;32m    639\u001b[0m     ssl_minimum_version\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mssl_minimum_version,\n\u001b[1;32m    640\u001b[0m     ssl_maximum_version\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mssl_maximum_version,\n\u001b[1;32m    641\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[1;32m    642\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[1;32m    643\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[1;32m    644\u001b[0m     cert_file\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[1;32m    645\u001b[0m     key_file\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[1;32m    646\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[1;32m    647\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    648\u001b[0m     ssl_context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mssl_context,\n\u001b[1;32m    649\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    650\u001b[0m     assert_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massert_hostname,\n\u001b[1;32m    651\u001b[0m     assert_fingerprint\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massert_fingerprint,\n\u001b[1;32m    652\u001b[0m )\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m sock_and_verified\u001b[39m.\u001b[39msocket\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/urllib3/connection.py:776\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    774\u001b[0m         server_hostname \u001b[39m=\u001b[39m normalized\n\u001b[0;32m--> 776\u001b[0m ssl_sock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    777\u001b[0m     sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    778\u001b[0m     keyfile\u001b[39m=\u001b[39;49mkey_file,\n\u001b[1;32m    779\u001b[0m     certfile\u001b[39m=\u001b[39;49mcert_file,\n\u001b[1;32m    780\u001b[0m     key_password\u001b[39m=\u001b[39;49mkey_password,\n\u001b[1;32m    781\u001b[0m     ca_certs\u001b[39m=\u001b[39;49mca_certs,\n\u001b[1;32m    782\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49mca_cert_dir,\n\u001b[1;32m    783\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49mca_cert_data,\n\u001b[1;32m    784\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    785\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[1;32m    786\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    787\u001b[0m )\n\u001b[1;32m    789\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/urllib3/util/ssl_.py:466\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n\u001b[1;32m    467\u001b[0m \u001b[39mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/urllib3/util/ssl_.py:510\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[39mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m--> 510\u001b[0m \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    512\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    515\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    518\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    519\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    520\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    521\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    522\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    523\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    524\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    525\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/ssl.py:1075\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1075\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1076\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/ssl.py:1346\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1345\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1346\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1347\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/urllib3/connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    490\u001b[0m         new_e \u001b[39m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[39m.\u001b[39mproxy\u001b[39m.\u001b[39mscheme)\n\u001b[0;32m--> 491\u001b[0m     \u001b[39mraise\u001b[39;00m new_e\n\u001b[1;32m    493\u001b[0m \u001b[39m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[39m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[0;31mSSLError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/urllib3/connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    842\u001b[0m     new_e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, new_e)\n\u001b[0;32m--> 844\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    845\u001b[0m     method, url, error\u001b[39m=\u001b[39;49mnew_e, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    846\u001b[0m )\n\u001b[1;32m    847\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[39m=\u001b[39m error \u001b[39mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[39mfrom\u001b[39;00m \u001b[39mreason\u001b[39;00m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /almanach/camembert-bio-base/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39malmanach/camembert-bio-base\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39malmanach/camembert-bio-base\u001b[39m\u001b[39m'\u001b[39m, num_labels\u001b[39m=\u001b[39mnb_labels)\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:582\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    581\u001b[0m \u001b[39m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 582\u001b[0m tokenizer_config \u001b[39m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    583\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    584\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tokenizer_config[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:433\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[39mLoads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[39mtokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    432\u001b[0m commit_hash \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 433\u001b[0m resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    434\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    435\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[1;32m    436\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    437\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    438\u001b[0m     resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    439\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    440\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    441\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    442\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    443\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    444\u001b[0m     _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    445\u001b[0m     _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    446\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    447\u001b[0m )\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m resolved_config_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    449\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    410\u001b[0m         path_or_repo_id,\n\u001b[1;32m    411\u001b[0m         filename,\n\u001b[1;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m         metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1196\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1197\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1198\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1199\u001b[0m             timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1200\u001b[0m         )\n\u001b[1;32m   1201\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1202\u001b[0m         \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m         commit_hash \u001b[39m=\u001b[39m http_error\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1532\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1529\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39mAccept-Encoding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39midentity\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1532\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1533\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1534\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1535\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1536\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1537\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1538\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1539\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1540\u001b[0m )\n\u001b[1;32m   1541\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1543\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:407\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39m# 2. Force relative redirection\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 407\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    408\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    409\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    410\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    411\u001b[0m         base_wait_time\u001b[39m=\u001b[39;49mbase_wait_time,\n\u001b[1;32m    412\u001b[0m         max_wait_time\u001b[39m=\u001b[39;49mmax_wait_time,\n\u001b[1;32m    413\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    414\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    415\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    418\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m300\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m399\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:442\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m    441\u001b[0m \u001b[39m# 3. Exponential backoff\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m \u001b[39mreturn\u001b[39;00m http_backoff(\n\u001b[1;32m    443\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    444\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    445\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    446\u001b[0m     base_wait_time\u001b[39m=\u001b[39;49mbase_wait_time,\n\u001b[1;32m    447\u001b[0m     max_wait_time\u001b[39m=\u001b[39;49mmax_wait_time,\n\u001b[1;32m    448\u001b[0m     retry_on_exceptions\u001b[39m=\u001b[39;49m(ConnectTimeout, ProxyError),\n\u001b[1;32m    449\u001b[0m     retry_on_status_codes\u001b[39m=\u001b[39;49m(),\n\u001b[1;32m    450\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    451\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    452\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:212\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    211\u001b[0m \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m response \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    214\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/.conda/envs/lesa_venv/lib/python3.11/site-packages/requests/adapters.py:517\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[39mraise\u001b[39;00m ProxyError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    515\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    519\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mSSLError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /almanach/camembert-bio-base/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('almanach/camembert-bio-base')\n",
    "config = AutoConfig.from_pretrained('almanach/camembert-bio-base', num_labels=nb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ad2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in range(num_labels):\n",
    "    for i in range(num_words_per_label):\n",
    "        label_inputs[label, i] = tokenizer.encode(key_words[label, i],\n",
    "                   add_special_tokens = False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6d681b",
   "metadata": {},
   "source": [
    "# Token Classification\n",
    "\n",
    "In the following cells we can either load the normal dataset or the blinded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afe27130-219f-471d-9b1c-49f3f5428525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "with open(\"../sadcsip/blind_test\", \"rb\") as fp:   # Unpickling\n",
    "   test_ds = pickle.load(fp)\n",
    "\n",
    "with open(\"../sadcsip/blind_val\", \"rb\") as fp:   # Unpickling\n",
    "   val_ds = pickle.load(fp)\n",
    "with open(\"../sadcsip/blind_train\", \"rb\") as fp:   # Unpickling\n",
    "   train_ds = pickle.load(fp)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d8cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open(\"../sadcsip/test\", \"rb\") as fp:   # Unpickling\n",
    "   test_ds = pickle.load(fp)\n",
    "\n",
    "with open(\"../sadcsip/val\", \"rb\") as fp:   # Unpickling\n",
    "   val_ds = pickle.load(fp)\n",
    "with open(\"../sadcsip/train\", \"rb\") as fp:   # Unpickling\n",
    "   train_ds = pickle.load(fp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced1de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = CustomizedDataset(test_ds)\n",
    "val_ds = CustomizedDataset(val_ds)\n",
    "train_ds = CustomizedDataset(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccda0624",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_inputs, tokenizer):\n",
    "    batch_size = len(examples)\n",
    "    text = [example[\"tokens\"] for example in examples]\n",
    "    tokenized_inputs = tokenizer(text, \n",
    "                                 padding=\"longest\", truncation=True, return_tensors=\"pt\",\n",
    "                                 is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i in range(len(examples)):\n",
    "        label = examples[i][\"classes\"]\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index = i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    tokenized_inputs[\"label_inputs\"] = label_inputs\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6b82e6",
   "metadata": {},
   "source": [
    "# Training token classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f06281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58d5eff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=24, \n",
    "    shuffle=False, \n",
    "    collate_fn=functools.partial(tokenize_and_align_labels, label_inputs=label_inputs, tokenizer=tokenizer)\n",
    ")\n",
    "#next(iter(val_dataloader))\n",
    "test_dataloader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=24,\n",
    "    shuffle=False,\n",
    "    collate_fn=functools.partial(tokenize_and_align_labels, label_inputs=label_inputs, tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc1efa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                                  | Params\n",
      "----------------------------------------------------------------\n",
      "0 | model | CamembertForTokenClassification_label | 110 M \n",
      "----------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.156   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 24. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 31/31 [00:09<00:00,  3.41it/s, v_num=977]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 17. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 31/31 [00:09<00:00,  3.14it/s, v_num=977, valid/f1=0.210, train/loss=1.910]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 12. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 31/31 [00:10<00:00,  3.02it/s, v_num=977, valid/f1=0.473, train/loss=0.00902]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/lomboa00/checkpoints/lesa_bert_camembert_tok_blind_only exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                                  | Params\n",
      "----------------------------------------------------------------\n",
      "0 | model | CamembertForTokenClassification_label | 110 M \n",
      "----------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.156   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 31/31 [00:10<00:00,  2.90it/s, v_num=978, valid/f1=0.425, train/loss=0.0138]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/lomboa00/checkpoints/lesa_bert_camembert_tok_blind_only exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                                  | Params\n",
      "----------------------------------------------------------------\n",
      "0 | model | CamembertForTokenClassification_label | 110 M \n",
      "----------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.156   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 31/31 [00:10<00:00,  3.00it/s, v_num=979, valid/f1=0.397, train/loss=0.0436]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/lomboa00/checkpoints/lesa_bert_camembert_tok_blind_only exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                                  | Params\n",
      "----------------------------------------------------------------\n",
      "0 | model | CamembertForTokenClassification_label | 110 M \n",
      "----------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.156   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 31/31 [00:10<00:00,  2.88it/s, v_num=980, valid/f1=0.342, train/loss=0.027] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/lomboa00/checkpoints/lesa_bert_camembert_tok_blind_only exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                                  | Params\n",
      "----------------------------------------------------------------\n",
      "0 | model | CamembertForTokenClassification_label | 110 M \n",
      "----------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.156   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 31/31 [00:11<00:00,  2.81it/s, v_num=981, valid/f1=0.310, train/loss=0.0921]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/lomboa00/checkpoints/lesa_bert_camembert_tok_blind_only exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                                  | Params\n",
      "----------------------------------------------------------------\n",
      "0 | model | CamembertForTokenClassification_label | 110 M \n",
      "----------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.156   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 31/31 [00:11<00:00,  2.65it/s, v_num=982, valid/f1=0.409, train/loss=0.0147]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/lomboa00/checkpoints/lesa_bert_camembert_tok_blind_only exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                                  | Params\n",
      "----------------------------------------------------------------\n",
      "0 | model | CamembertForTokenClassification_label | 110 M \n",
      "----------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.156   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 31/31 [00:11<00:00,  2.69it/s, v_num=983, valid/f1=0.333, train/loss=0.0662]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/lomboa00/checkpoints/lesa_bert_camembert_tok_blind_only exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                                  | Params\n",
      "----------------------------------------------------------------\n",
      "0 | model | CamembertForTokenClassification_label | 110 M \n",
      "----------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.156   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 31/31 [00:12<00:00,  2.51it/s, v_num=984, valid/f1=0.290, train/loss=0.155]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/lomboa00/checkpoints/lesa_bert_camembert_tok_blind_only exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                                  | Params\n",
      "----------------------------------------------------------------\n",
      "0 | model | CamembertForTokenClassification_label | 110 M \n",
      "----------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.156   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 31/31 [00:12<00:00,  2.44it/s, v_num=985, valid/f1=0.366, train/loss=0.0169]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/lomboa00/checkpoints/lesa_bert_camembert_tok_blind_only exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                                  | Params\n",
      "----------------------------------------------------------------\n",
      "0 | model | CamembertForTokenClassification_label | 110 M \n",
      "----------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.156   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 31/31 [00:12<00:00,  2.48it/s, v_num=986, valid/f1=0.378, train/loss=0.0185]\n"
     ]
    }
   ],
   "source": [
    "seeds = [10, 100, 1000, 10000, 100000, 12, 123, 1234, 12345, 123456]\n",
    "for rep in range(len(seeds)):\n",
    "    torch.manual_seed(seeds[rep])\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds, \n",
    "        batch_size=24, \n",
    "        shuffle=True, \n",
    "        num_workers = 8,\n",
    "        collate_fn=functools.partial(tokenize_and_align_labels, label_inputs=label_inputs, tokenizer=tokenizer)\n",
    "    )\n",
    "    camembert_tok = LightningModel(model_name = 'LESA_BertForTokClassif', optim=\"AdamW\", lr=3e-5, weight_decay=0.01)\n",
    "    model_checkpoint = pl.callbacks.ModelCheckpoint(#dirpath = '../checkpoints/lesa_bert_camembert_tok',\n",
    "                                                    dirpath = '../checkpoints/lesa_bert_camembert_tok_blind',\n",
    "                                                    monitor=\"valid/f1\", mode=\"max\")\n",
    "\n",
    "    camembert_tok_trainer = pl.Trainer(\n",
    "        callbacks=[\n",
    "            pl.callbacks.EarlyStopping(monitor=\"valid/f1\", patience=4, mode=\"max\"),\n",
    "            model_checkpoint,\n",
    "        ]\n",
    "    )\n",
    "    camembert_tok_trainer.fit(camembert_tok, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df926478-f97e-4b6d-a2b2-e6969a120e36",
   "metadata": {},
   "source": [
    "## Collecting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e4cd8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "label_names = ['O', 'Cp', 'FC', 'D', 'SO2', 'AGPR', 'G', 'CI']\n",
    "nb_labels = len(label_names)\n",
    "\n",
    "prediction_trainer = pl.Trainer(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61a28c04-8a56-4fd9-aa80-7277f89e9415",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1_scores=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf256bc3-ecc3-40fd-bfb7-9463c6606b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 11.13it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 11.07it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 11.24it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['layer_norm.bias', 'layer_norm.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/lomboa00/.conda/envs/lesa_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 11.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = '../checkpoints/lesa_bert_camembert_tok_blind'\n",
    "#path = '../checkpoints/lesa_bert_camembert_tok'\n",
    "\n",
    "for filename in os.listdir(path):    \n",
    "    checkpoint_path = os.path.join(path, filename)\n",
    "    camembert_tok_classifier = LightningModel.load_from_checkpoint(checkpoint_path= checkpoint_path)\n",
    "    camembert_preds = prediction_trainer.predict(camembert_tok_classifier, dataloaders=test_dataloader)\n",
    "    camembert_preds = [batch_preds.view(-1) for batch_preds in camembert_preds]\n",
    "    camembert_preds = torch.cat(camembert_preds, -1)\n",
    "    # collecting the labels\n",
    "    labels = []\n",
    "    it = iter(test_dataloader)\n",
    "    exit = False\n",
    "    while not exit:\n",
    "        try:\n",
    "            # Samples the batch\n",
    "            tokenized_batch = next(it)\n",
    "            label = tokenized_batch['labels']\n",
    "            labels.append(label.view(-1))\n",
    "        except StopIteration:\n",
    "            exit = True\n",
    "    labels = torch.cat(labels, -1)\n",
    "    \n",
    "    f1_score = custom_f1_score(labels, camembert_preds, num_labels, average= None)\n",
    "    f1_scores.append(f1_score)\n",
    "f1_scores = np.array(f1_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "161da2fb-c0d4-4aa5-b7b3-261bf7642eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99508938, 0.91555549, 0.83344767, 0.80695026, 0.90349748,\n",
       "       0.95003421, 0.25607726, 0.89086681])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## blind dataset\n",
    "average, std = stats(f1_scores)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37787a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00144552, 0.07620634, 0.0815513 , 0.1403761 , 0.01567572,\n",
       "       0.0906814 , 0.04364542, 0.05710715])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdb345c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99599058, 0.84888882, 0.86955222, 0.83618849, 0.91365418,\n",
       "       0.99787229, 0.27400339, 0.91321975])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## normal dataset\n",
    "average, std = stats(f1_scores)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfb6d3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00072312, 0.07013215, 0.04245362, 0.13097443, 0.02375537,\n",
       "       0.00638298, 0.01321255, 0.05175449])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5d3b68-b8cd-44c7-a216-4b16e10abe61",
   "metadata": {},
   "source": [
    "## Some illustrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9fcb553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_extended_attention_mask(attention_mask):\n",
    "    # Provided a padding mask of dimensions [batch_size, seq_length]\n",
    "    # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "    extended_attention_mask = attention_mask[:, None, None, :]\n",
    "    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "    # masked positions, this operation will create a tensor which is 0.0 for\n",
    "    # positions we want to attend and the dtype's smallest value for masked positions.\n",
    "    # Since we are adding it to the raw scores before the softmax, this is\n",
    "    # effectively the same as removing these entirely.\n",
    "    extended_attention_mask = extended_attention_mask.to(dtype=torch.float)  # fp16 compatibility\n",
    "    extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(torch.float).min\n",
    "    return extended_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e19696d-0e07-49c3-8ece-595fa4bc98c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#selection of the batch\n",
    "it = iter(test_dataloader)\n",
    "tokenized_batch = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0101d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_label: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_label from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_label were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['classifier.bias', 'classifier.weight', 'layer_norm.bias', 'layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = '../checkpoints/lesa_bert_camembert_tok/epoch=0-step=74.ckpt'\n",
    "\n",
    "camembert_tok = LightningModel.load_from_checkpoint(checkpoint_path= checkpoint_path).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e01210c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "camembert_tok.eval()\n",
    "model_output = camembert_tok.model.roberta(\n",
    "            input_ids = tokenized_batch[\"input_ids\"].cuda(),\n",
    "            label_inputs = tokenized_batch['label_inputs'].cuda(),\n",
    "            attention_mask = tokenized_batch[\"attention_mask\"].cuda(),\n",
    "            output_hidden_states=True\n",
    ")\n",
    "sentences = decode_properly(tokenized_batch[\"input_ids\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25e7ec61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_padding_mask = tokenized_batch['attention_mask'].type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ed625a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_padding_mask = get_extended_attention_mask(key_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10d4040d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_id = 11\n",
    "sentence_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5aa91df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entry_layer, entry_label_embeddings = model_output[\"hidden_states\"][layer_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4965c9e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, (attn_output_weights, _, _, _) = camembert_tok.model.roberta.encoder.layer[layer_id].attention.self(\n",
    "                                    entry_layer,\n",
    "                                    attention_mask= key_padding_mask.cuda(), \n",
    "                                    output_attentions=True,\n",
    "                                    label_embeddings=entry_label_embeddings,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "138d1267",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "head_id = 4\n",
    "attn_output_weights_visual = attn_output_weights[sentence_id, head_id, :12, :12].detach().cpu().numpy()\n",
    "token_names = tokenizer.convert_ids_to_tokens(tokenized_batch[\"input_ids\"][sentence_id][:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ed679",
   "metadata": {},
   "source": [
    "Now the self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c6129696",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(\n",
    "        attn_output_weights_visual,\n",
    "        annot = False, \n",
    "        cbar=False,\n",
    "        #fmt=\"d\",\n",
    "        xticklabels=token_names,\n",
    "        yticklabels=token_names,\n",
    "        cmap=\"viridis\"\n",
    ")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lesa_venv",
   "language": "python",
   "name": "lesa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
